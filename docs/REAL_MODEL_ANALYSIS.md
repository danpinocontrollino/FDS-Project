# Real Model Training Issue - Analysis & Presentation Strategy

## ðŸ” What We Discovered

The "real model" trained on StudentLife data outputs **constant predictions**:
- Stress: 5.0 (always)
- Mood: 6.0 (always)  
- Anxiety: 8.0 (always)
- Energy: 6.0 (always)

These are the **default values** used to fill missing data during training.

## ðŸ› Root Cause

**Line 313-315 in `train_studentlife_lstm_kaggle.py`:**

```python
'stress_level': stress_daily.get(date, 5.0),  # Defaults to 5.0 if no EMA
'mood_score': mood_daily.get(date, 6.0),      # Defaults to 6.0 if no EMA
'anxiety_score': 8.0,                          # ALWAYS 8.0 (no extraction code)
'energy_level': 6.0,                           # ALWAYS 6.0 (no extraction code)
```

**The Reality:**
- StudentLife has **674 days** of behavioral data (sleep, activity, social, etc.)
- But only **~10 days** have actual EMA mental health responses
- The rest (98.5%) are filled with defaults
- **Result**: Model learned to predict defaults, not patterns

## ðŸ“Š Data Sparsity Breakdown

| Data Type | Coverage | Quality |
|-----------|----------|---------|
| Behavioral (sleep, exercise, screen time) | 674/674 days (100%) | âœ… Excellent - real sensors |
| Mental Health EMA (stress, mood, anxiety) | ~10/674 days (1.5%) | âŒ Too sparse - survey fatigue |
| Training labels | 674/674 days | âš ï¸ 98.5% are defaults, not real |

## ðŸŽ¯ Turn This Into a Strength for Presentation

This is actually **PERFECT** for your narrative! Here's why:

### Option 1: Honest Discovery Story (RECOMMENDED)

**Narrative:**
> "We wanted to compare synthetic vs real training data. We found a real behavioral dataset (StudentLife) and trained a model. But we discovered something important: the model just predicted defaults! Why? Because even 'real' data can have quality issues - in this case, missing labels. This taught us that data quality has TWO dimensions:
> 1. **Feature quality**: Real behavioral correlations (StudentLife âœ…, Synthetic âŒ)
> 2. **Label quality**: Frequent mental health surveys (StudentLife âŒ, Synthetic âœ…)
> 
> Neither dataset is perfect. This is a common real-world challenge in mental health ML."

**Why This Works:**
- Shows critical thinking and scientific rigor
- Demonstrates you understand ML limitations
- More impressive than if everything "just worked"
- Real research involves unexpected findings

### Option 2: The "Label Scarcity" Angle

**Narrative:**
> "Our comparison revealed a fundamental challenge in mental health prediction: **the label scarcity problem**. Even with perfect behavioral sensors, models fail without frequent mental health assessments. This explains why many mental health apps struggle - they have rich sensor data but users don't complete enough self-reports."

**Academic Framing:**
- This is a known problem in mHealth research
- You can cite papers on "survey fatigue" and "missing EMA data"
- Makes your project more research-oriented

### Option 3: Focus on What DID Work

**Keep the comparison but shift focus:**
- Show the synthetic model DOES vary its predictions based on behavior
- Show the comparison visualizations (they're still valid)
- Mention the real model limitation briefly as a "future work" item
- Focus on the data quality visualizations (correlation differences)

## ðŸ“ˆ What Your Visualizations Still Show (All Valid!)

1. **Synthetic model predictions vary** based on behavioral inputs âœ…
2. **Confrontation dashboard** shows the comparison methodology âœ…
3. **Data quality charts** show correlation differences (this is the REAL insight) âœ…
4. **You have working code** for dual model comparison (reusable framework) âœ…

## ðŸš€ Presentation Recommendations

### Slide Structure:

**Slide 1: "Data Quality Investigation"**
- We discovered synthetic data has weak correlations
- Needed to compare against real-world data

**Slide 2: "StudentLife Dataset"**
- Real behavioral sensors from 49 students
- Rich feature variance vs synthetic monotony
- [Show feature variance chart]

**Slide 3: "The Label Scarcity Challenge"**
- Trained model on StudentLife
- Discovered: 98.5% of labels are missing/defaults
- Model learned defaults, not patterns
- [Show constant predictions]

**Slide 4: "Key Insight: Two Dimensions of Data Quality"**
```
           Feature Quality
              HIGH    LOW
Label    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Quality  â”‚ Ideal   â”‚ Student â”‚
HIGH     â”‚ (rare)  â”‚ Life    â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
LOW      â”‚Syntheticâ”‚ Worst   â”‚
         â”‚ Kaggle  â”‚ (both)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Slide 5: "Implications for Mental Health ML"**
- Sensor data alone insufficient
- Need engagement strategies for self-reports
- Our synthetic model shows what's POSSIBLE with labels
- Future work: Find/create dataset with both

## ðŸ”§ If You Want to Fix It (Optional)

To actually train a working real model, you'd need:

1. **Use ONLY days with actual EMA responses** (10 days total)
   - Too few for training
   - But you could show overfitting

2. **Extract anxiety/depression from other sources**
   - StudentLife has PAM (Photographic Affect Meter)
   - Has conversation audio features
   - Could use as proxy labels

3. **Find different dataset**
   - Tesserae (Rochester)
   - TILES (USC)
   - Might have better label coverage

## ðŸ’¡ Bottom Line

**Don't hide this - feature it!** Scientific integrity is showing:
1. What you tried
2. What you found (even if unexpected)
3. What you learned
4. How you'd improve

Professors LOVE seeing students encounter real research challenges and respond thoughtfully. This makes your project more authentic, not less.

---

## ðŸ“ Updated Abstract Language

**Before:**
> "We compared two models to show training data quality matters"

**After:**
> "We investigated the impact of training data quality on mental health predictions. Our comparison revealed that data quality encompasses both feature correlations and label availability. While StudentLife provides real behavioral patterns, extreme label sparsity (1.5% coverage) prevented effective learning. This highlights a fundamental challenge in mental health ML: the trade-off between synthetic data with complete labels but weak correlations, versus real data with strong correlations but missing labels."

**This is better research!** ðŸŽ“
