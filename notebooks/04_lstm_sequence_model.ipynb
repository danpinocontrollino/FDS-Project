{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c6c8234",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Sequence Models - LSTM, GRU, CNN\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "Questo notebook implementa **modelli sequenziali** per predire il burnout da finestre temporali di 7 giorni. A differenza dell'MLP (che usa features aggregate), qui il modello vede la **sequenza giorno per giorno**.\n",
    "\n",
    "### PerchÃ© Modelli Sequenziali?\n",
    "Il burnout si sviluppa nel tempo. Una singola giornata non cattura il trend:\n",
    "- Declino progressivo della qualitÃ  del sonno\n",
    "- Accumulo di stress settimana dopo settimana\n",
    "- Pattern ciclici lavoro-recupero\n",
    "\n",
    "### Architetture Implementate\n",
    "\n",
    "#### 1. LSTM (Long Short-Term Memory)\n",
    "- Reti ricorrenti con \"memoria\" a lungo termine\n",
    "- Gates (forget, input, output) controllano il flusso di informazione\n",
    "- Best per: pattern che si estendono su piÃ¹ giorni\n",
    "\n",
    "#### 2. GRU (Gated Recurrent Unit)\n",
    "- Versione semplificata dell'LSTM\n",
    "- Meno parametri, spesso performance simile\n",
    "- Best per: dataset piÃ¹ piccoli o training piÃ¹ veloce\n",
    "\n",
    "#### 3. CNN 1D (Convolutional Neural Network)\n",
    "- Filtri convoluzionali su sequenze temporali\n",
    "- Cattura pattern locali (2-3 giorni consecutivi)\n",
    "- Best per: pattern locali ripetuti\n",
    "\n",
    "### Input\n",
    "- `data/processed/daily_with_burnout.parquet` (dati giornalieri con burnout label)\n",
    "\n",
    "### Output\n",
    "- `models/saved/lstm_classifier.pt`, `gru_classifier.pt`, `cnn1d_classifier.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT E CONFIGURAZIONE\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device selection\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data/processed')\n",
    "MODEL_DIR = Path('../models/saved')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ace76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARICAMENTO DATI GIORNALIERI\n",
    "# =============================================================================\n",
    "# Carichiamo i dati giornalieri con il burnout_level giÃ  calcolato\n",
    "# (generato da scripts/create_burnout_labels.py)\n",
    "\n",
    "daily_path = DATA_DIR / 'daily_with_burnout.parquet'\n",
    "daily = pd.read_parquet(daily_path)\n",
    "\n",
    "# Assicuriamoci che i dati siano ordinati per utente e data\n",
    "daily['date'] = pd.to_datetime(daily['date'])\n",
    "daily = daily.sort_values(['user_id', 'date'])\n",
    "\n",
    "# IMPORTANTE: Converti work_pressure da stringa a numerico\n",
    "# Il dataset originale ha valori \"low\", \"medium\", \"high\"\n",
    "if daily['work_pressure'].dtype == object:\n",
    "    pressure_map = {'low': 0, 'medium': 1, 'high': 2}\n",
    "    daily['work_pressure'] = daily['work_pressure'].map(pressure_map).fillna(1).astype(np.float32)\n",
    "\n",
    "# Features per il modello sequenziale (15 metriche giornaliere)\n",
    "feature_cols = [\n",
    "    'sleep_hours', 'sleep_quality',      # Sonno\n",
    "    'work_hours', 'meetings_count',      # Lavoro\n",
    "    'tasks_completed', 'exercise_minutes', 'steps_count',  # AttivitÃ \n",
    "    'caffeine_mg', 'alcohol_units', 'screen_time_hours',  # Consumo\n",
    "    'stress_level', 'mood_score', 'energy_level', 'focus_score',  # Psicologici\n",
    "    'work_pressure'  # Ambiente (ora numerico)\n",
    "]\n",
    "\n",
    "# Dimensione finestra: 7 giorni (una settimana lavorativa)\n",
    "window = 7\n",
    "print(f\"Features: {len(feature_cols)}, Window: {window} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece92a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREAZIONE SEQUENZE (SLIDING WINDOW)\n",
    "# =============================================================================\n",
    "# Per ogni utente, creiamo finestre scorrevoli di 7 giorni.\n",
    "# La label Ã¨ il burnout dell'ULTIMO giorno della finestra.\n",
    "#\n",
    "# Esempio per un utente con 10 giorni:\n",
    "#   Seq 1: giorni 1-7  â†’ label = giorno 7\n",
    "#   Seq 2: giorni 2-8  â†’ label = giorno 8\n",
    "#   Seq 3: giorni 3-9  â†’ label = giorno 9\n",
    "#   Seq 4: giorni 4-10 â†’ label = giorno 10\n",
    "\n",
    "def build_sequences(df, features, window):\n",
    "    \"\"\"\n",
    "    Crea sequenze sliding window dai dati giornalieri.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con dati giornalieri\n",
    "        features: lista di colonne feature\n",
    "        window: dimensione finestra in giorni\n",
    "    \n",
    "    Returns:\n",
    "        X: array (N_seq, window, N_features)\n",
    "        y: array (N_seq,) con labels\n",
    "    \"\"\"\n",
    "    sequences, labels = [], []\n",
    "    \n",
    "    for uid, group in df.groupby('user_id'):\n",
    "        feats = group[features].to_numpy(dtype=np.float32)\n",
    "        labs = group['burnout_level'].to_numpy(dtype=np.int64)\n",
    "        \n",
    "        # Skip utenti con meno di `window` giorni\n",
    "        if len(group) < window:\n",
    "            continue\n",
    "        \n",
    "        # Sliding window\n",
    "        for idx in range(window, len(group) + 1):\n",
    "            seq = feats[idx - window: idx]  # 7 giorni di features\n",
    "            label = labs[idx - 1]            # Burnout dell'ultimo giorno\n",
    "            \n",
    "            # Skip se ci sono NaN\n",
    "            if np.isnan(seq).any():\n",
    "                continue\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.stack(sequences), np.array(labels)\n",
    "\n",
    "# Costruiamo le sequenze\n",
    "seq_X, seq_y = build_sequences(daily, feature_cols, window)\n",
    "print(f\"Total sequences: {len(seq_X):,}\")\n",
    "print(f\"Shape: {seq_X.shape} = (sequences, days, features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdd688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN/VAL SPLIT E DATALOADER\n",
    "# =============================================================================\n",
    "\n",
    "# Split stratificato per mantenere proporzione classi\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    seq_X, seq_y, test_size=0.2, stratify=seq_y, random_state=42\n",
    ")\n",
    "\n",
    "# Wrapping in TensorDataset\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "# DataLoader con batching\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "# Dimensioni per i modelli\n",
    "seq_len = X_train.shape[1]   # 7\n",
    "input_dim = X_train.shape[2]  # 15\n",
    "num_classes = len(np.unique(seq_y))  # 3\n",
    "print(f\"seq_len={seq_len}, input_dim={input_dim}, num_classes={num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ARCHITETTURE DEI MODELLI\n",
    "# =============================================================================\n",
    "\n",
    "class SequenceNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Rete ricorrente (LSTM o GRU) per classificazione sequenze.\n",
    "    \n",
    "    Architettura:\n",
    "        Input (batch, 7, 15) \n",
    "            â†’ LSTM/GRU 2 layers (hidden=128)\n",
    "            â†’ Prendi ultimo hidden state\n",
    "            â†’ LayerNorm â†’ ReLU â†’ Dropout â†’ Linear(3)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, cell='lstm'):\n",
    "        super().__init__()\n",
    "        # Selezione tipo di cella ricorrente\n",
    "        rnn_cls = nn.LSTM if cell == 'lstm' else nn.GRU\n",
    "        \n",
    "        # 2 layer RNN con dropout tra i layer\n",
    "        self.rnn = rnn_cls(\n",
    "            input_dim, hidden_dim, \n",
    "            batch_first=True,  # Input: (batch, seq, features)\n",
    "            num_layers=2, \n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)       # out: (batch, seq_len, hidden)\n",
    "        last = out[:, -1, :]       # Prendi ultimo timestep\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN 1D per classificazione sequenze.\n",
    "    \n",
    "    Architettura:\n",
    "        Input (batch, 7, 15) â†’ transpose â†’ (batch, 15, 7)\n",
    "            â†’ Conv1d(64, kernel=3) â†’ ReLU â†’ BatchNorm\n",
    "            â†’ Conv1d(128, kernel=3) â†’ ReLU â†’ GlobalAvgPool\n",
    "            â†’ Dropout â†’ Linear(3)\n",
    "    \n",
    "    I filtri convoluzionali catturano pattern locali (2-3 giorni).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Conv1d: input_dim canali â†’ 64 canali\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            # 64 â†’ 128 canali\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Global average pooling: (batch, 128, seq) â†’ (batch, 128, 1)\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        # Conv1d vuole: (batch, channels, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        feats = self.conv(x)\n",
    "        return self.fc(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNZIONE DI TRAINING\n",
    "# =============================================================================\n",
    "# Funzione generica per allenare qualsiasi modello sequenziale\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=40, lr=1e-3, name='model'):\n",
    "    \"\"\"\n",
    "    Training loop standard per modelli sequenziali.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader, val_loader: DataLoaders\n",
    "        epochs: numero di epoche\n",
    "        lr: learning rate\n",
    "        name: nome per salvare il checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        history: dict con train/val loss per epoca\n",
    "    \"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    history = {'train': [], 'val': []}\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # === Training ===\n",
    "        model.train()\n",
    "        tr_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_losses.append(loss.item())\n",
    "        \n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                val_losses.append(criterion(model(xb), yb).item())\n",
    "        \n",
    "        train_loss = np.mean(tr_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save({\n",
    "                'state_dict': model.state_dict(), \n",
    "                'feature_cols': feature_cols\n",
    "            }, MODEL_DIR / f'{name}.pt')\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"[{name}] Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716231bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING DEI 3 MODELLI\n",
    "# =============================================================================\n",
    "# Alleniamo LSTM, GRU e CNN1D per confrontare le architetture\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "hist_lstm = train_model(\n",
    "    SequenceNet(input_dim, cell='lstm'), \n",
    "    train_loader, val_loader, \n",
    "    name='lstm_classifier'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining GRU...\")\n",
    "hist_gru = train_model(\n",
    "    SequenceNet(input_dim, cell='gru'), \n",
    "    train_loader, val_loader, \n",
    "    name='gru_classifier'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining CNN1D...\")\n",
    "hist_cnn = train_model(\n",
    "    CNN1D(input_dim, seq_len), \n",
    "    train_loader, val_loader, \n",
    "    name='cnn1d_classifier'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LEARNING CURVES\n",
    "# =============================================================================\n",
    "# Confronto visivo dell'andamento del training per i 3 modelli\n",
    "\n",
    "def plot_history(history, title):\n",
    "    \"\"\"Plotta train/val loss.\"\"\"\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(history['train'], label='Train')\n",
    "    plt.plot(history['val'], label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_history(hist_lstm, 'LSTM Training Curves')\n",
    "plot_history(hist_gru, 'GRU Training Curves')\n",
    "plot_history(hist_cnn, 'CNN1D Training Curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c24128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALUTAZIONE FINALE\n",
    "# =============================================================================\n",
    "# Carichiamo i best models e confrontiamo le metriche\n",
    "\n",
    "def evaluate_model(model_path):\n",
    "    \"\"\"Carica un modello e calcola predizioni sul validation set.\"\"\"\n",
    "    payload = torch.load(model_path, map_location=DEVICE, weights_only=False)\n",
    "    state = payload['state_dict']\n",
    "    name = model_path.stem\n",
    "    \n",
    "    # Ricostruisci l'architettura corretta\n",
    "    if 'lstm' in name:\n",
    "        model = SequenceNet(input_dim, cell='lstm')\n",
    "    elif 'gru' in name:\n",
    "        model = SequenceNet(input_dim, cell='gru')\n",
    "    else:\n",
    "        model = CNN1D(input_dim, seq_len)\n",
    "    \n",
    "    model.load_state_dict(state)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        xb = torch.from_numpy(X_val).to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    return preds\n",
    "\n",
    "# Valutazione di tutti i modelli\n",
    "print(\"=== SEQUENCE MODELS COMPARISON ===\\n\")\n",
    "for model_name in ['lstm_classifier.pt', 'gru_classifier.pt', 'cnn1d_classifier.pt']:\n",
    "    preds = evaluate_model(MODEL_DIR / model_name)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    print(f\"{model_name:25s} Accuracy: {acc:.4f}, F1 Macro: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
