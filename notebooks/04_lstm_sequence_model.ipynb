{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c6c8234",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Sequence Models - LSTM, GRU, CNN\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook implements **sequence models** to predict burnout from 7-day temporal windows. Unlike MLP (which uses aggregated features), here the model sees the **sequence day by day**.\n",
    "\n",
    "### Why Sequence Models?\n",
    "Burnout develops over time. A single day doesn't capture the trend:\n",
    "- Progressive decline in sleep quality\n",
    "- Stress accumulation week after week\n",
    "- Cyclical work-recovery patterns\n",
    "\n",
    "### Implemented Architectures\n",
    "\n",
    "#### 1. LSTM (Long Short-Term Memory)\n",
    "- Recurrent networks with long-term \"memory\"\n",
    "- Gates (forget, input, output) control information flow\n",
    "- Best for: patterns spanning multiple days\n",
    "\n",
    "#### 2. GRU (Gated Recurrent Unit)\n",
    "- Simplified version of LSTM\n",
    "- Fewer parameters, often similar performance\n",
    "- Best for: smaller datasets or faster training\n",
    "\n",
    "#### 3. CNN 1D (Convolutional Neural Network)\n",
    "- Convolutional filters on temporal sequences\n",
    "- Captures local patterns (2-3 consecutive days)\n",
    "- Best for: repeated local patterns\n",
    "\n",
    "### Input\n",
    "- `data/processed/daily_with_burnout.parquet` (daily data with burnout label)\n",
    "\n",
    "### Output\n",
    "- `models/saved/lstm_classifier.pt`, `gru_classifier.pt`, `cnn1d_classifier.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device selection\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data/processed')\n",
    "MODEL_DIR = Path('../models/saved')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ace76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD DAILY DATA\n",
    "# =============================================================================\n",
    "# Load daily data with burnout_level already calculated\n",
    "# (generated by scripts/create_burnout_labels.py)\n",
    "\n",
    "daily_path = DATA_DIR / 'daily_with_burnout.parquet'\n",
    "daily = pd.read_parquet(daily_path)\n",
    "\n",
    "# Ensure data is sorted by user and date\n",
    "daily['date'] = pd.to_datetime(daily['date'])\n",
    "daily = daily.sort_values(['user_id', 'date'])\n",
    "\n",
    "# IMPORTANT: Convert work_pressure from string to numeric\n",
    "# Original dataset has values \"low\", \"medium\", \"high\"\n",
    "if daily['work_pressure'].dtype == object:\n",
    "    pressure_map = {'low': 0, 'medium': 1, 'high': 2}\n",
    "    daily['work_pressure'] = daily['work_pressure'].map(pressure_map).fillna(1).astype(np.float32)\n",
    "\n",
    "# Features for sequence model (15 daily metrics)\n",
    "feature_cols = [\n",
    "    'sleep_hours', 'sleep_quality',      # Sleep\n",
    "    'work_hours', 'meetings_count',      # Work\n",
    "    'tasks_completed', 'exercise_minutes', 'steps_count',  # Activity\n",
    "    'caffeine_mg', 'alcohol_units', 'screen_time_hours',  # Consumption\n",
    "    'stress_level', 'mood_score', 'energy_level', 'focus_score',  # Psychological\n",
    "    'work_pressure'  # Environment (now numeric)\n",
    "]\n",
    "\n",
    "# Window size: 7 days (a work week)\n",
    "window = 7\n",
    "print(f\"Features: {len(feature_cols)}, Window: {window} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece92a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEQUENCE CREATION (SLIDING WINDOW)\n",
    "# =============================================================================\n",
    "# For each user, create sliding windows of 7 days.\n",
    "# Label is the burnout of the LAST day of the window.\n",
    "#\n",
    "# Example for a user with 10 days:\n",
    "#   Seq 1: days 1-7  â†’ label = day 7\n",
    "#   Seq 2: days 2-8  â†’ label = day 8\n",
    "#   Seq 3: days 3-9  â†’ label = day 9\n",
    "#   Seq 4: days 4-10 â†’ label = day 10\n",
    "\n",
    "def build_sequences(df, features, window):\n",
    "    \"\"\"\n",
    "    Create sliding window sequences from daily data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with daily data\n",
    "        features: list of feature columns\n",
    "        window: window size in days\n",
    "    \n",
    "    Returns:\n",
    "        X: array (N_seq, window, N_features)\n",
    "        y: array (N_seq,) with labels\n",
    "    \"\"\"\n",
    "    sequences, labels = [], []\n",
    "    \n",
    "    for uid, group in df.groupby('user_id'):\n",
    "        feats = group[features].to_numpy(dtype=np.float32)\n",
    "        labs = group['burnout_level'].to_numpy(dtype=np.int64)\n",
    "        \n",
    "        # Skip users with fewer than `window` days\n",
    "        if len(group) < window:\n",
    "            continue\n",
    "        \n",
    "        # Sliding window\n",
    "        for idx in range(window, len(group) + 1):\n",
    "            seq = feats[idx - window: idx]  # 7 days of features\n",
    "            label = labs[idx - 1]            # Burnout of last day\n",
    "            \n",
    "            # Skip if there are NaN values\n",
    "            if np.isnan(seq).any():\n",
    "                continue\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.stack(sequences), np.array(labels)\n",
    "\n",
    "# Build sequences\n",
    "seq_X, seq_y = build_sequences(daily, feature_cols, window)\n",
    "print(f\"Total sequences: {len(seq_X):,}\")\n",
    "print(f\"Shape: {seq_X.shape} = (sequences, days, features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdd688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN/VAL SPLIT AND DATALOADER\n",
    "# =============================================================================\n",
    "\n",
    "# Stratified split to maintain class proportions\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    seq_X, seq_y, test_size=0.2, stratify=seq_y, random_state=42\n",
    ")\n",
    "\n",
    "# Wrap in TensorDataset\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "# DataLoader with batching\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "# Dimensions for models\n",
    "seq_len = X_train.shape[1]   # 7\n",
    "input_dim = X_train.shape[2]  # 15\n",
    "num_classes = len(np.unique(seq_y))  # 3\n",
    "print(f\"seq_len={seq_len}, input_dim={input_dim}, num_classes={num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL ARCHITECTURES\n",
    "# =============================================================================\n",
    "\n",
    "class SequenceNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent network (LSTM or GRU) for sequence classification.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (batch, 7, 15) \n",
    "            â†’ LSTM/GRU 2 layers (hidden=128)\n",
    "            â†’ Take last hidden state\n",
    "            â†’ LayerNorm â†’ ReLU â†’ Dropout â†’ Linear(3)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, cell='lstm'):\n",
    "        super().__init__()\n",
    "        # Select recurrent cell type\n",
    "        rnn_cls = nn.LSTM if cell == 'lstm' else nn.GRU\n",
    "        \n",
    "        # 2-layer RNN with dropout between layers\n",
    "        self.rnn = rnn_cls(\n",
    "            input_dim, hidden_dim, \n",
    "            batch_first=True,  # Input: (batch, seq, features)\n",
    "            num_layers=2, \n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)       # out: (batch, seq_len, hidden)\n",
    "        last = out[:, -1, :]       # Take last timestep\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN for sequence classification.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (batch, 7, 15) â†’ transpose â†’ (batch, 15, 7)\n",
    "            â†’ Conv1d(64, kernel=3) â†’ ReLU â†’ BatchNorm\n",
    "            â†’ Conv1d(128, kernel=3) â†’ ReLU â†’ GlobalAvgPool\n",
    "            â†’ Dropout â†’ Linear(3)\n",
    "    \n",
    "    Convolutional filters capture local patterns (2-3 days).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Conv1d: input_dim channels â†’ 64 channels\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            # 64 â†’ 128 channels\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Global average pooling: (batch, 128, seq) â†’ (batch, 128, 1)\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        # Conv1d expects: (batch, channels, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        feats = self.conv(x)\n",
    "        return self.fc(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "# Generic function to train any sequence model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=40, lr=1e-3, name='model'):\n",
    "    \"\"\"\n",
    "    Standard training loop for sequence models.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader, val_loader: DataLoaders\n",
    "        epochs: number of epochs\n",
    "        lr: learning rate\n",
    "        name: name for saving checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        history: dict with train/val loss per epoch\n",
    "    \"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    history = {'train': [], 'val': []}\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # === Training ===\n",
    "        model.train()\n",
    "        tr_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_losses.append(loss.item())\n",
    "        \n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                val_losses.append(criterion(model(xb), yb).item())\n",
    "        \n",
    "        train_loss = np.mean(tr_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save({\n",
    "                'state_dict': model.state_dict(), \n",
    "                'feature_cols': feature_cols\n",
    "            }, MODEL_DIR / f'{name}.pt')\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"[{name}] Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716231bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING ALL 3 MODELS\n",
    "# =============================================================================\n",
    "# Train LSTM, GRU and CNN1D to compare architectures\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "hist_lstm = train_model(\n",
    "    SequenceNet(input_dim, cell='lstm'), \n",
    "    train_loader, val_loader, \n",
    "    name='lstm_classifier'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining GRU...\")\n",
    "hist_gru = train_model(\n",
    "    SequenceNet(input_dim, cell='gru'), \n",
    "    train_loader, val_loader, \n",
    "    name='gru_classifier'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining CNN1D...\")\n",
    "hist_cnn = train_model(\n",
    "    CNN1D(input_dim, seq_len), \n",
    "    train_loader, val_loader, \n",
    "    name='cnn1d_classifier'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LEARNING CURVES\n",
    "# =============================================================================\n",
    "# Visual comparison of training progress for all 3 models\n",
    "\n",
    "def plot_history(history, title):\n",
    "    \"\"\"Plot train/val loss.\"\"\"\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(history['train'], label='Train')\n",
    "    plt.plot(history['val'], label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_history(hist_lstm, 'LSTM Training Curves')\n",
    "plot_history(hist_gru, 'GRU Training Curves')\n",
    "plot_history(hist_cnn, 'CNN1D Training Curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c24128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL EVALUATION\n",
    "# =============================================================================\n",
    "# Load best models and compare metrics\n",
    "\n",
    "def evaluate_model(model_path):\n",
    "    \"\"\"Load a model and compute predictions on validation set.\"\"\"\n",
    "    payload = torch.load(model_path, map_location=DEVICE, weights_only=False)\n",
    "    state = payload['state_dict']\n",
    "    name = model_path.stem\n",
    "    \n",
    "    # Reconstruct correct architecture\n",
    "    if 'lstm' in name:\n",
    "        model = SequenceNet(input_dim, cell='lstm')\n",
    "    elif 'gru' in name:\n",
    "        model = SequenceNet(input_dim, cell='gru')\n",
    "    else:\n",
    "        model = CNN1D(input_dim, seq_len)\n",
    "    \n",
    "    model.load_state_dict(state)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        xb = torch.from_numpy(X_val).to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    return preds\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"=== SEQUENCE MODELS COMPARISON ===\\n\")\n",
    "for model_name in ['lstm_classifier.pt', 'gru_classifier.pt', 'cnn1d_classifier.pt']:\n",
    "    preds = evaluate_model(MODEL_DIR / model_name)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    print(f\"{model_name:25s} Accuracy: {acc:.4f}, F1 Macro: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
