{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81325f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "KAGGLE = os.path.exists('/kaggle/input')\n",
    "BASE_PATH = '/kaggle/input/student-life/dataset' if KAGGLE else 'data/studentlife'\n",
    "OUTPUT_PATH = '/kaggle/working' if KAGGLE else 'reports'\n",
    "print(f\"Environment: {'Kaggle' if KAGGLE else 'Local'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data extraction functions\n",
    "\n",
    "def get_all_student_ids(base_path):\n",
    "    \"\"\"Find student IDs with better path detection\"\"\"\n",
    "    student_ids = set()\n",
    "    \n",
    "    # Try multiple possible paths\n",
    "    possible_paths = [\n",
    "        os.path.join(base_path, 'sensing', 'phonelock'),\n",
    "        os.path.join(base_path, 'phonelock'),\n",
    "        base_path\n",
    "    ]\n",
    "    \n",
    "    for sensing_path in possible_paths:\n",
    "        if os.path.exists(sensing_path):\n",
    "            print(f\"  Checking: {sensing_path}\")\n",
    "            for f in os.listdir(sensing_path):\n",
    "                if f.startswith('phonelock_u') and f.endswith('.csv'):\n",
    "                    student_ids.add(f.replace('phonelock_', '').replace('.csv', ''))\n",
    "            if student_ids:\n",
    "                break\n",
    "    \n",
    "    if not student_ids:\n",
    "        print(f\"‚ö†Ô∏è  No students found. Available paths:\")\n",
    "        for p in possible_paths:\n",
    "            print(f\"    {p} - Exists: {os.path.exists(p)}\")\n",
    "        if os.path.exists(base_path):\n",
    "            print(f\"  Contents of {base_path}:\")\n",
    "            for item in os.listdir(base_path)[:10]:\n",
    "                print(f\"    - {item}\")\n",
    "    \n",
    "    return sorted(list(student_ids))\n",
    "\n",
    "def extract_sleep_from_phonelock(base_path, student_id):\n",
    "    phonelock_path = os.path.join(base_path, 'sensing', 'phonelock', f'phonelock_{student_id}.csv')\n",
    "    if not os.path.exists(phonelock_path): return {}\n",
    "    try:\n",
    "        df = pd.read_csv(phonelock_path, encoding='utf-8-sig')\n",
    "        if 'start' not in df.columns: return {}\n",
    "        df['start_dt'] = pd.to_datetime(df['start'], unit='s')\n",
    "        df['duration_hours'] = (df['end'] - df['start']) / 3600\n",
    "        df['date'] = df['start_dt'].dt.date\n",
    "        df['start_hour'] = df['start_dt'].dt.hour\n",
    "        daily_sleep = {}\n",
    "        for date in df['date'].unique():\n",
    "            day_data = df[df['date'] == date]\n",
    "            night = day_data[((day_data['start_hour'] >= 22) | (day_data['start_hour'] <= 10)) & (day_data['duration_hours'] >= 3)]\n",
    "            if len(night) > 0: daily_sleep[date] = min(night['duration_hours'].max(), 12)\n",
    "        return daily_sleep\n",
    "    except: return {}\n",
    "\n",
    "def extract_activity(base_path, student_id):\n",
    "    path = os.path.join(base_path, 'sensing', 'activity', f'activity_{student_id}.csv')\n",
    "    if not os.path.exists(path): return {}, {}\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "        if 'timestamp' not in df.columns: return {}, {}\n",
    "        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df['date'] = df['datetime'].dt.date\n",
    "        act_col = ' activity inference' if ' activity inference' in df.columns else 'activity_inference'\n",
    "        if act_col not in df.columns: return {}, {}\n",
    "        exercise, steps = {}, {}\n",
    "        for date in df['date'].unique():\n",
    "            day = df[df['date'] == date]\n",
    "            active = day[day[act_col].isin([1, 2])]\n",
    "            mins = len(active) * 3 / 60\n",
    "            exercise[date] = min(mins, 120)\n",
    "            steps[date] = int(mins * 100)\n",
    "        return exercise, steps\n",
    "    except: return {}, {}\n",
    "\n",
    "def extract_screen_time(base_path, student_id):\n",
    "    path = os.path.join(base_path, 'sensing', 'phonelock', f'phonelock_{student_id}.csv')\n",
    "    if not os.path.exists(path): return {}\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "        df['start_dt'] = pd.to_datetime(df['start'], unit='s')\n",
    "        df['date'] = df['start_dt'].dt.date\n",
    "        df['locked'] = (df['end'] - df['start']) / 3600\n",
    "        screen = {}\n",
    "        for date in df['date'].unique():\n",
    "            day = df[df['date'] == date]\n",
    "            screen[date] = min(max(0, 18 - day['locked'].sum()), 16)\n",
    "        return screen\n",
    "    except: return {}\n",
    "\n",
    "def extract_social(base_path, student_id):\n",
    "    path = os.path.join(base_path, 'sensing', 'conversation', f'conversation_{student_id}.csv')\n",
    "    social = {}\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "            col = 'start_timestamp' if 'start_timestamp' in df.columns else ' start_timestamp'\n",
    "            if col in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df[col], unit='s')\n",
    "                df['date'] = df['datetime'].dt.date\n",
    "                for date in df['date'].unique():\n",
    "                    social[date] = len(df[df['date'] == date])\n",
    "        except: pass\n",
    "    return social\n",
    "\n",
    "def extract_work(base_path, student_id):\n",
    "    path = os.path.join(base_path, 'app_usage', f'running_app_{student_id}.csv')\n",
    "    if not os.path.exists(path): return {}\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df['date'] = df['datetime'].dt.date\n",
    "        pkg_col = 'RUNNING_TASKS_topActivity_mPackage'\n",
    "        if pkg_col not in df.columns: return {}\n",
    "        work_pkgs = ['com.google', 'edu.', 'blackboard', 'chrome', 'microsoft']\n",
    "        work = {}\n",
    "        for date in df['date'].unique():\n",
    "            day = df[df['date'] == date]\n",
    "            mask = day[pkg_col].astype(str).apply(lambda x: any(p in x.lower() for p in work_pkgs))\n",
    "            work[date] = min(len(day[mask]) * 10 / 3600, 14)\n",
    "        return work\n",
    "    except: return {}\n",
    "\n",
    "print(\"Data extraction functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Build dataset with smart imputation\n",
    "\n",
    "def build_dataset(base_path):\n",
    "    student_ids = get_all_student_ids(base_path)\n",
    "    print(f\"Found {len(student_ids)} students\")\n",
    "    \n",
    "    if len(student_ids) == 0:\n",
    "        print(\"‚ùå No students found - cannot proceed\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    records = []\n",
    "    for i, sid in enumerate(student_ids):\n",
    "        sleep = extract_sleep_from_phonelock(base_path, sid)\n",
    "        exercise, steps = extract_activity(base_path, sid)\n",
    "        screen = extract_screen_time(base_path, sid)\n",
    "        social = extract_social(base_path, sid)\n",
    "        work = extract_work(base_path, sid)\n",
    "        \n",
    "        all_dates = set()\n",
    "        for d in [sleep, exercise, screen, social, work]: all_dates.update(d.keys())\n",
    "        \n",
    "        for date in sorted(all_dates):\n",
    "            records.append({\n",
    "                'student_id': sid, 'date': date,\n",
    "                'sleep_hours': sleep.get(date, np.nan),\n",
    "                'exercise_minutes': exercise.get(date, np.nan),\n",
    "                'steps_count': steps.get(date, np.nan),\n",
    "                'screen_time_hours': screen.get(date, np.nan),\n",
    "                'social_interactions': social.get(date, np.nan),\n",
    "                'work_hours': work.get(date, np.nan),\n",
    "            })\n",
    "        if (i+1) % 10 == 0: print(f\"  Processed {i+1}/{len(student_ids)}\")\n",
    "    \n",
    "    if not records:\n",
    "        print(\"‚ö†Ô∏è  No records extracted\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['student_id', 'date']).reset_index(drop=True)\n",
    "    print(f\"\\n‚úì Dataset: {len(df)} records, {df['student_id'].nunique()} students\")\n",
    "    \n",
    "    # Report missing data before imputation\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).round(1)\n",
    "    print(f\"\\nüìä Missing data:\")\n",
    "    for col, pct in missing_pct.items():\n",
    "        if pct > 0 and col not in ['student_id', 'date']:\n",
    "            print(f\"  {col}: {pct}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = build_dataset(BASE_PATH)\n",
    "if len(df) > 0:\n",
    "    df.head()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Empty dataset - check paths above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8430951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Smart missing data imputation + sequence creation\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "\n",
    "def smart_impute_student_data(df, feature_cols):\n",
    "    \"\"\"Multi-strategy imputation: temporal patterns + KNN + iterative modeling\"\"\"\n",
    "    \n",
    "    print(\"\\nüß† Smart imputation pipeline:\")\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Strategy 1: Forward/Backward fill per student (temporal continuity)\n",
    "    print(\"  1. Temporal fill (forward/backward per student)...\")\n",
    "    for sid in df_imputed['student_id'].unique():\n",
    "        mask = df_imputed['student_id'] == sid\n",
    "        for col in feature_cols:\n",
    "            df_imputed.loc[mask, col] = df_imputed.loc[mask, col].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    still_missing = df_imputed[feature_cols].isnull().sum().sum()\n",
    "    print(f\"     ‚Üí {still_missing} values still missing\")\n",
    "    \n",
    "    # Strategy 2: Rolling mean (7-day window per student)\n",
    "    if still_missing > 0:\n",
    "        print(\"  2. Rolling mean imputation (7-day window)...\")\n",
    "        for sid in df_imputed['student_id'].unique():\n",
    "            mask = df_imputed['student_id'] == sid\n",
    "            sdata = df_imputed[mask].copy()\n",
    "            for col in feature_cols:\n",
    "                rolling_mean = sdata[col].rolling(window=7, min_periods=1, center=True).mean()\n",
    "                df_imputed.loc[mask, col] = df_imputed.loc[mask, col].fillna(rolling_mean)\n",
    "        \n",
    "        still_missing = df_imputed[feature_cols].isnull().sum().sum()\n",
    "        print(f\"     ‚Üí {still_missing} values still missing\")\n",
    "    \n",
    "    # Strategy 3: KNN Imputation (find similar days from other students)\n",
    "    if still_missing > 0:\n",
    "        print(\"  3. KNN imputation (k=5, similar days across students)...\")\n",
    "        knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "        df_imputed[feature_cols] = knn_imputer.fit_transform(df_imputed[feature_cols])\n",
    "        \n",
    "        still_missing = df_imputed[feature_cols].isnull().sum().sum()\n",
    "        print(f\"     ‚Üí {still_missing} values still missing\")\n",
    "    \n",
    "    # Strategy 4: Iterative Imputation (predict missing using Random Forest)\n",
    "    if still_missing > 0:\n",
    "        print(\"  4. Iterative imputation (Random Forest predictor)...\")\n",
    "        iter_imputer = IterativeImputer(max_iter=10, random_state=42, verbose=0)\n",
    "        df_imputed[feature_cols] = iter_imputer.fit_transform(df_imputed[feature_cols])\n",
    "        \n",
    "        still_missing = df_imputed[feature_cols].isnull().sum().sum()\n",
    "        print(f\"     ‚Üí {still_missing} values remaining\")\n",
    "    \n",
    "    # Strategy 5: Global median fallback (last resort)\n",
    "    if still_missing > 0:\n",
    "        print(\"  5. Global median fallback (last resort)...\")\n",
    "        for col in feature_cols:\n",
    "            df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
    "    \n",
    "    # Clip to realistic ranges\n",
    "    df_imputed['sleep_hours'] = df_imputed['sleep_hours'].clip(2, 14)\n",
    "    df_imputed['exercise_minutes'] = df_imputed['exercise_minutes'].clip(0, 180)\n",
    "    df_imputed['steps_count'] = df_imputed['steps_count'].clip(0, 30000)\n",
    "    df_imputed['screen_time_hours'] = df_imputed['screen_time_hours'].clip(0, 18)\n",
    "    df_imputed['social_interactions'] = df_imputed['social_interactions'].clip(0, 50)\n",
    "    df_imputed['work_hours'] = df_imputed['work_hours'].clip(0, 16)\n",
    "    \n",
    "    print(f\"‚úì Imputation complete - 0 missing values\")\n",
    "    return df_imputed\n",
    "\n",
    "def create_sequences(df, feature_cols, seq_length=7):\n",
    "    \"\"\"Create sequences from imputed data\"\"\"\n",
    "    sequences, targets, sids, dates = [], [], [], []\n",
    "    \n",
    "    # Smart imputation first\n",
    "    df_clean = smart_impute_student_data(df, feature_cols)\n",
    "    \n",
    "    for sid in df_clean['student_id'].unique():\n",
    "        sdata = df_clean[df_clean['student_id'] == sid].sort_values('date').copy()\n",
    "        if len(sdata) < seq_length + 1: continue\n",
    "        \n",
    "        vals = sdata[feature_cols].values\n",
    "        dvals = sdata['date'].values\n",
    "        \n",
    "        for i in range(len(vals) - seq_length):\n",
    "            sequences.append(vals[i:i+seq_length])\n",
    "            targets.append(vals[i+seq_length])\n",
    "            sids.append(sid)\n",
    "            dates.append(dvals[i+seq_length])\n",
    "    \n",
    "    X, y = np.array(sequences), np.array(targets)\n",
    "    print(f\"\\n‚úì Created {len(X)} sequences from {df_clean['student_id'].nunique()} students\")\n",
    "    return X, y, sids, dates\n",
    "\n",
    "feature_cols = ['sleep_hours', 'exercise_minutes', 'steps_count', \n",
    "               'screen_time_hours', 'social_interactions', 'work_hours']\n",
    "\n",
    "if len(df) > 0:\n",
    "    X, y, student_ids, dates = create_sequences(df, feature_cols)\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping sequence creation - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58293ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Model definitions\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.name = \"LSTM\"\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, \n",
    "                           dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.name = \"BiLSTM\"\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, \n",
    "                           bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, input_dim)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.name = \"GRU\"\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                         dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, nhead=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.name = \"Transformer\"\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, dropout)\n",
    "        layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, \n",
    "                                          dim_feedforward=hidden_dim*4, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(self.input_proj(x))\n",
    "        return self.fc(self.transformer(x)[:, -1, :])\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.name = \"CNN-LSTM\"\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.dropout(self.relu(self.conv1(x)))\n",
    "        x = self.relu(self.conv2(x)).permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, seq_length=7, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.name = \"MLP\"\n",
    "        self.fc1 = nn.Linear(input_dim * seq_length, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x.view(x.size(0), -1))))\n",
    "        return self.fc3(self.dropout(self.relu(self.fc2(x))))\n",
    "\n",
    "print(\"Model definitions ready: LSTM, BiLSTM, GRU, Transformer, CNN-LSTM, MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00628366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training utilities\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=50, lr=0.001, patience=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_batch), y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                val_loss += criterion(model(X_batch.to(device)), y_batch.to(device)).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: break\n",
    "    \n",
    "    if best_state: model.load_state_dict(best_state)\n",
    "    return model, best_val_loss\n",
    "\n",
    "def run_cv(model_class, model_kwargs, X, y, n_folds=5, epochs=50):\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Normalize\n",
    "        X_mean, X_std = X_train.mean(axis=(0,1)), X_train.std(axis=(0,1)) + 1e-8\n",
    "        y_mean, y_std = y_train.mean(axis=0), y_train.std(axis=0) + 1e-8\n",
    "        \n",
    "        X_train_n = (X_train - X_mean) / X_std\n",
    "        X_val_n = (X_val - X_mean) / X_std\n",
    "        y_train_n = (y_train - y_mean) / y_std\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train_n), \n",
    "                                                torch.FloatTensor(y_train_n)), batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val_n), \n",
    "                                              torch.FloatTensor((y_val - y_mean) / y_std)), batch_size=32)\n",
    "        \n",
    "        model = model_class(**model_kwargs)\n",
    "        model, _ = train_model(model, train_loader, val_loader, epochs=epochs)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            device = next(model.parameters()).device\n",
    "            preds_n = model(torch.FloatTensor(X_val_n).to(device)).cpu().numpy()\n",
    "        preds = preds_n * y_std + y_mean\n",
    "        \n",
    "        results.append({\n",
    "            'mae': mean_absolute_error(y_val, preds),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_val, preds)),\n",
    "            'r2': r2_score(y_val, preds)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'mae_mean': np.mean([r['mae'] for r in results]),\n",
    "        'mae_std': np.std([r['mae'] for r in results]),\n",
    "        'rmse_mean': np.mean([r['rmse'] for r in results]),\n",
    "        'r2_mean': np.mean([r['r2'] for r in results]),\n",
    "        'r2_std': np.std([r['r2'] for r in results]),\n",
    "    }\n",
    "\n",
    "print(\"Training utilities ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: RUN MODEL COMPARISON EXPERIMENT\n",
    "\n",
    "input_dim = len(feature_cols)\n",
    "hidden_dim = 64\n",
    "\n",
    "models = [\n",
    "    (\"LSTM\", LSTMModel, {'input_dim': input_dim, 'hidden_dim': hidden_dim}),\n",
    "    (\"BiLSTM\", BiLSTMModel, {'input_dim': input_dim, 'hidden_dim': hidden_dim}),\n",
    "    (\"GRU\", GRUModel, {'input_dim': input_dim, 'hidden_dim': hidden_dim}),\n",
    "    (\"Transformer\", TransformerModel, {'input_dim': input_dim, 'hidden_dim': hidden_dim}),\n",
    "    (\"CNN-LSTM\", CNNLSTMModel, {'input_dim': input_dim, 'hidden_dim': hidden_dim}),\n",
    "    (\"MLP\", MLPModel, {'input_dim': input_dim, 'hidden_dim': 128, 'seq_length': 7}),\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON (5-fold CV)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {}\n",
    "for name, model_class, kwargs in models:\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    try:\n",
    "        res = run_cv(model_class, kwargs, X, y, n_folds=5, epochs=50)\n",
    "        results[name] = res\n",
    "        print(f\"  R¬≤: {res['r2_mean']:.4f} ¬± {res['r2_std']:.4f}\")\n",
    "        print(f\"  MAE: {res['mae_mean']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "\n",
    "# Rank by R¬≤\n",
    "ranking = sorted(results.items(), key=lambda x: x[1]['r2_mean'], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÜ LEADERBOARD\")\n",
    "print(\"=\"*70)\n",
    "for i, (name, res) in enumerate(ranking, 1):\n",
    "    medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else \"  \"\n",
    "    print(f\"{medal} {i}. {name:<15} R¬≤={res['r2_mean']:.4f}  MAE={res['mae_mean']:.4f}\")\n",
    "\n",
    "best_name = ranking[0][0]\n",
    "print(f\"\\n‚úÖ BEST MODEL: {best_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e7add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train and save best model\n",
    "\n",
    "# Get best model class\n",
    "best_class, best_kwargs = None, None\n",
    "for name, cls, kwargs in models:\n",
    "    if name == best_name:\n",
    "        best_class, best_kwargs = cls, kwargs\n",
    "        break\n",
    "\n",
    "# Normalize all data\n",
    "X_mean = X.mean(axis=(0, 1))\n",
    "X_std = X.std(axis=(0, 1)) + 1e-8\n",
    "y_mean = y.mean(axis=0)\n",
    "y_std = y.std(axis=0) + 1e-8\n",
    "\n",
    "X_norm = (X - X_mean) / X_std\n",
    "y_norm = (y - y_mean) / y_std\n",
    "\n",
    "# 90/10 split\n",
    "n = int(0.9 * len(X))\n",
    "train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_norm[:n]), \n",
    "                                        torch.FloatTensor(y_norm[:n])), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_norm[n:]), \n",
    "                                      torch.FloatTensor(y_norm[n:])), batch_size=32)\n",
    "\n",
    "print(f\"Training final {best_name} model...\")\n",
    "final_model = best_class(**best_kwargs)\n",
    "final_model, _ = train_model(final_model, train_loader, val_loader, epochs=100)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'model_name': best_name,\n",
    "    'model_state': final_model.state_dict(),\n",
    "    'model_kwargs': best_kwargs,\n",
    "    'feature_cols': feature_cols,\n",
    "    'scaler_mean_X': X_mean.tolist(),\n",
    "    'scaler_std_X': X_std.tolist(),\n",
    "    'scaler_mean_y': y_mean.tolist(),\n",
    "    'scaler_std_y': y_std.tolist(),\n",
    "    'cv_results': results[best_name],\n",
    "    'all_results': {k: {kk: vv for kk, vv in v.items()} for k, v in results.items()},\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, f'{OUTPUT_PATH}/best_behavioral_model.pt')\n",
    "print(f\"\\n‚úì Saved: best_behavioral_model.pt\")\n",
    "print(f\"  Model: {best_name}\")\n",
    "print(f\"  R¬≤: {results[best_name]['r2_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16530dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save results JSON\n",
    "\n",
    "results_json = {\n",
    "    'experiment_date': datetime.now().isoformat(),\n",
    "    'dataset': {\n",
    "        'total_sequences': len(X),\n",
    "        'num_students': len(set(student_ids)),\n",
    "        'features': feature_cols\n",
    "    },\n",
    "    'models_tested': list(results.keys()),\n",
    "    'results': {k: v for k, v in results.items()},\n",
    "    'ranking': [(name, res['r2_mean']) for name, res in ranking],\n",
    "    'best_model': {\n",
    "        'name': best_name,\n",
    "        'r2': results[best_name]['r2_mean'],\n",
    "        'mae': results[best_name]['mae_mean']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/model_comparison_results.json', 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "print(\"‚úì Saved: model_comparison_results.json\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDownload:\")\n",
    "print(\"  1. best_behavioral_model.pt\")\n",
    "print(\"  2. model_comparison_results.json\")\n",
    "print(\"\\nNext: Combine with synthetic model for two-stage pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
