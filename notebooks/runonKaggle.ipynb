{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ced6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# KAGGLE GPU DRIVER NOTEBOOK - Burnout Prediction Training Pipeline\n",
    "# ============================================================================\n",
    "# This notebook runs the full training pipeline on Kaggle's GPU environment\n",
    "# Dataset: Work-Life Balance Synthetic Daily Wellness Dataset\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215fb02",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "Clone the repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5369c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fds-project'...\n",
      "remote: Enumerating objects: 214, done.\u001b[K\n",
      "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
      "remote: Enumerating objects: 214, done.\u001b[K\u001b[K\n",
      "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
      "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
      "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
      "remote: Total 214 (delta 82), reused 161 (delta 50), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (214/214), 8.82 MiB | 22.68 MiB/s, done.\n",
      "remote: Total 214 (delta 82), reused 161 (delta 50), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (214/214), 8.82 MiB | 22.68 MiB/s, done.\n",
      "Resolving deltas: 100% (82/82), done.\n",
      "Resolving deltas: 100% (82/82), done.\n",
      "/workspaces/FDS-Project/notebooks/fds-project\n",
      "/workspaces/FDS-Project/notebooks/fds-project\n"
     ]
    }
   ],
   "source": [
    "# Clone the GitHub repository\n",
    "!git clone https://github.com/danpinocontrollino/fds-project.git\n",
    "\n",
    "# Change working directory to the cloned repo\n",
    "%cd fds-project\n",
    "\n",
    "# Install required libraries (not pre-installed on Kaggle)\n",
    "!pip install -q joblib pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5d3e4",
   "metadata": {},
   "source": [
    "## Step 2: Data Setup\n",
    "Copy the Kaggle dataset into the expected `data/raw/` directory.\n",
    "\n",
    "**Important:** Kaggle input paths are read-only, so we must copy (not symlink) the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08b2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying dataset files from Kaggle input...\n",
      "\n",
      "Files in data/raw:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggle's input path for the dataset\n",
    "KAGGLE_INPUT = Path(\"/kaggle/input/worklife-balance-synthetic-daily-wellness-dataset\")\n",
    "\n",
    "# Target directory in the cloned repo\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy all CSV files from Kaggle input to data/raw/\n",
    "print(\"Copying dataset files from Kaggle input...\")\n",
    "for csv_file in KAGGLE_INPUT.glob(\"*.csv\"):\n",
    "    dest = RAW_DIR / csv_file.name\n",
    "    shutil.copy(csv_file, dest)\n",
    "    print(f\"  ✓ {csv_file.name} -> {dest}\")\n",
    "\n",
    "# Verify the files were copied\n",
    "print(f\"\\nFiles in {RAW_DIR}:\")\n",
    "for f in RAW_DIR.iterdir():\n",
    "    print(f\"  - {f.name} ({f.stat().st_size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74324afb",
   "metadata": {},
   "source": [
    "## Step 3: Pipeline Execution\n",
    "Run the preprocessing scripts to generate burnout labels and prepare features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48516830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Running: create_burnout_labels.py\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/FDS-Project/notebooks/fds-project/scripts/create_burnout_labels.py\", line 209, in <module>\n",
      "    main()\n",
      "  File \"/workspaces/FDS-Project/notebooks/fds-project/scripts/create_burnout_labels.py\", line 187, in main\n",
      "    weekly, daily = load_raw_frames()\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/FDS-Project/notebooks/fds-project/scripts/create_burnout_labels.py\", line 68, in load_raw_frames\n",
      "    weekly = pd.read_csv(RAW_DIR / \"weekly_summaries.csv\", parse_dates=[\"week_start\"])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/FDS-Project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/FDS-Project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/FDS-Project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/FDS-Project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"/workspaces/FDS-Project/.venv/lib/python3.12/site-packages/pandas/io/common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/raw/weekly_summaries.csv'\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/raw/weekly_summaries.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 3a: Generate burnout labels from raw data\n",
    "print(\"=\" * 60)\n",
    "print(\"Running: create_burnout_labels.py\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/create_burnout_labels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408bc967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Running: preprocess.py\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/FDS-Project/notebooks/fds-project/scripts/preprocess.py\", line 364, in <module>\n",
      "    main()\n",
      "  File \"/workspaces/FDS-Project/notebooks/fds-project/scripts/preprocess.py\", line 348, in main\n",
      "    ensure_inputs_exist()\n",
      "  File \"/workspaces/FDS-Project/notebooks/fds-project/scripts/preprocess.py\", line 123, in ensure_inputs_exist\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Missing processed parquet files. Run scripts/create_burnout_labels.py first: data/processed/daily_with_burnout.parquet, data/processed/weekly_with_burnout.parquet\n"
     ]
    }
   ],
   "source": [
    "# Step 3b: Preprocess features for MLP model\n",
    "print(\"=\" * 60)\n",
    "print(\"Running: preprocess.py\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75644c8c",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "Train all models using GPU acceleration.\n",
    "\n",
    "**Models:**\n",
    "- MLP (Multi-Layer Perceptron) - tabular baseline\n",
    "- LSTM (Long Short-Term Memory) - captures temporal dependencies\n",
    "- GRU (Gated Recurrent Unit) - lighter variant of LSTM\n",
    "- Transformer - attention-based, parallel processing\n",
    "\n",
    "**Configuration:**\n",
    "- Window: 7 days (weekly patterns)\n",
    "- Epochs: 40\n",
    "- Sample Users: 100% (use full dataset with GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14df20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP model (tabular baseline)\n",
    "print(\"=\" * 60)\n",
    "print(\"Training: MLP Classifier (Tabular)\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/train_mlp.py --epochs 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training: Transformer Sequence Model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train_lstm.py [-h] [--model {lstm,gru,cnn}] [--window WINDOW]\n",
      "                     [--epochs EPOCHS] [--batch-size BATCH_SIZE] [--lr LR]\n",
      "                     [--sample-users SAMPLE_USERS]\n",
      "train_lstm.py: error: argument --model: invalid choice: 'transformer' (choose from lstm, gru, cnn)\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model\n",
    "print(\"=\" * 60)\n",
    "print(\"Training: LSTM Sequence Model\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/train_lstm.py --model lstm --window 7 --epochs 40 --sample-users 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da638049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU model\n",
    "print(\"=\" * 60)\n",
    "print(\"Training: GRU Sequence Model\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/train_lstm.py --model gru --window 7 --epochs 40 --sample-users 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dfbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer model\n",
    "print(\"=\" * 60)\n",
    "print(\"Training: Transformer Sequence Model\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/train_transformer.py --window 7 --epochs 40 --sample-users 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4e86f",
   "metadata": {},
   "source": [
    "## Step 5: MAE Pre-training & Fine-tuning\n",
    "Train a Masked Autoencoder (self-supervised) then fine-tune for classification.\n",
    "\n",
    "**MAE learns behavioral patterns by reconstructing masked days.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51083298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5a: MAE Pre-training (self-supervised)\n",
    "print(\"=\" * 60)\n",
    "print(\"MAE Pre-training: Learning behavioral patterns\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/train_mae.py --epochs 50 --sample-users 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ba0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5b: Fine-tune MAE for classification\n",
    "print(\"=\" * 60)\n",
    "print(\"MAE Fine-tuning: Transfer learning for burnout classification\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/train_mae_classifier.py --epochs 30 --sample-users 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cde3c2",
   "metadata": {},
   "source": [
    "## Step 6: CVAE Smart Advisor\n",
    "Train a Conditional VAE that can suggest lifestyle changes to reduce burnout.\n",
    "\n",
    "**Generates \"counterfactual\" schedules: \"What would your week look like with low burnout?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b84c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CVAE Smart Advisor\n",
    "print(\"=\" * 60)\n",
    "print(\"Training: CVAE Smart Advisor (Generative Model)\")\n",
    "print(\"=\" * 60)\n",
    "!python scripts/train_cvae.py --epochs 100 --sample-users 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9cdf8",
   "metadata": {},
   "source": [
    "## Step 7: Save Models to Kaggle Output\n",
    "Copy all trained models to `/kaggle/working/` so they persist after the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ba555ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/kaggle/working/models'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/kaggle/working'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m MODEL_DIR = Path(\u001b[33m\"\u001b[39m\u001b[33mmodels/saved\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m OUTPUT_DIR = Path(\u001b[33m\"\u001b[39m\u001b[33m/kaggle/working/models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving trained models to Kaggle output...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_file \u001b[38;5;129;01min\u001b[39;00m MODEL_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33m*.pt\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/pathlib.py:1315\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1314\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/pathlib.py:1315\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1314\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1307\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1308\u001b[39m \u001b[33;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[32m   1309\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '/kaggle'"
     ]
    }
   ],
   "source": [
    "# Copy trained models to Kaggle's output directory\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"models/saved\")\n",
    "OUTPUT_DIR = Path(\"/kaggle/working/models\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saving trained models to Kaggle output...\")\n",
    "for model_file in MODEL_DIR.glob(\"*.pt\"):\n",
    "    dest = OUTPUT_DIR / model_file.name\n",
    "    shutil.copy(model_file, dest)\n",
    "    print(f\"  ✓ Saved: {dest}\")\n",
    "\n",
    "print(f\"\\n✅ Models saved! Download from Kaggle's 'Output' tab.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
